{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9a85351-1b15-49b4-a704-73f394da913f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in d:\\anaconda\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: requests in d:\\anaconda\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: pandas in d:\\anaconda\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: lxml in d:\\anaconda\\lib\\site-packages (5.2.1)\n",
      "Requirement already satisfied: fake_useragent in d:\\anaconda\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in d:\\anaconda\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\anaconda\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anaconda\\lib\\site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\lib\\site-packages (from requests) (2024.8.30)\n",
      "Requirement already satisfied: numpy>=1.26.0 in d:\\anaconda\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\anaconda\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\anaconda\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\anaconda\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in d:\\anaconda\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: mysql-connector-python in d:\\anaconda\\lib\\site-packages (9.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4 requests pandas lxml fake_useragent\n",
    "!pip install mysql-connector-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b60e22c4-3641-4c11-a5c6-07bb938d7981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scraped Data:\n",
      "\n",
      "                                                                                                                                                Title                                            Author  Publication Date Rating Price\n",
      "                                                                                           \"Mastering the Data Paradox: Key to Winning in the AI Age\"                                        Nitin Seth    April 10, 2024    4.8 17.00\n",
      "                               \"Data Engineering with AWS: Acquire the skills to design and build AWS-based data transformation pipelines like a pro\"                                      Gareth Eagar  October 31, 2023    4.3 24.00\n",
      "\"Cracking the Data Engineering Interview: Land your dream job with the help of resume-building tips, over 100 mock questions, and a unique portfolio\"                                    Kedeisha Bryan  November 7, 2023    4.0 26.00\n",
      "                                                                                            \"Hands-On Data Engineering with R, Python and PostgreSQL\"                                   Michel Ballings December 14, 2024   None 69.00\n",
      "                                                                               \"Fundamentals of Data Engineering: Plan and Build Robust Data Systems\"                            Joe Reis, Matt Housley     July 26, 2022    4.7 43.00\n",
      "                                           \"Designing Data-Intensive Applications: The Big Ideas Behind Reliable, Scalable, and Maintainable Systems\"                                  Martin Kleppmann     April 2, 2017    4.8 23.00\n",
      "                               \"Data Engineering with AWS: Acquire the skills to design and build AWS-based data transformation pipelines like a pro\"                                      Gareth Eagar  October 31, 2023    4.3 24.00\n",
      "                                                                                       \"AI Engineering: Building Applications with Foundation Models\"                                        Chip Huyen   January 7, 2025    4.6 67.00\n",
      "                                               \"Data Engineering Best Practices: Architect robust and cost-effective data solutions in the cloud era\"             Richard J. Schiller, David Larochelle  October 11, 2024    5.0 39.00\n",
      "                                                                        \"Financial Data Engineering: Design and Build Data-Driven Financial Products\"                                    Tamer Khraisha November 12, 2024    5.0 60.00\n",
      "                                                    \"Data Engineering Design Patterns: Recipes for Solving the Most Common Data Engineering Problems\"                                              None      June 3, 2025   None 79.00\n",
      "                                                                          \"Data Pipelines Pocket Reference: Moving and Processing Data for Analytics\"                                    James Densmore    March 16, 2021    4.6 17.00\n",
      " \"The Data Engineering Handbook: We are Data Engineers, we make things happen, we pull rabbits out of hats, and transform raw, noisy data into gold.\"                                              None  October 17, 2024    5.0  9.00\n",
      "                    \"Data Engineering with Databricks Cookbook: Build effective data and AI solutions using Apache Spark, Databricks, and Delta Lake\"                                     Pulkit Chadha      May 31, 2024    4.4 39.00\n",
      "                                                                      \"Storytelling with Data: A Data Visualization Guide for Business Professionals\"                           Cole Nussbaumer Knaflic  November 2, 2015    4.6 26.00\n",
      "\"Cracking the Data Engineering Interview: Land your dream job with the help of resume-building tips, over 100 mock questions, and a unique portfolio\"                                    Kedeisha Bryan  November 7, 2023    4.0 26.00\n",
      "                                                                                                                          \"Data Engineering on Azure\"                                     Vlad Riscutia   August 17, 2021    4.5 43.00\n",
      "                                                   \"Managing Data as a Product: Design and build data-product-centered socio-technical architectures\"                                      Andrea Gioia November 29, 2024    4.4 44.00\n",
      "                                                         \"Data Engineering with Alteryx: Helping data engineers apply DataOps practices with Alteryx\"                                     Paul Houghton     June 30, 2022    4.3 31.00\n",
      "                  \"Getting Started with DuckDB: A practical guide for accelerating your data science, data analytics, and data engineering workflows\"                         Simon Aubury, Ned Letcher     June 24, 2024    3.4 35.00\n",
      "                            \"Data Engineering with Python: Work with massive datasets to design data models and automate data pipelines using Python\"                                     Paul Crickard  October 23, 2020    4.1 37.00\n",
      "                                                              \"Data-Driven Science and Engineering: Machine Learning, Dynamical Systems, and Control\"                 Steven L. Brunton, J. Nathan Kutz     July 28, 2022    4.8 60.00\n",
      "                                          \"Ace the Data Science Interview: 201 Real Interview Questions Asked By FAANG, Tech Startups, & Wall Street\"                             Nick Singh, Kevin Huo   August 16, 2021    4.5 42.00\n",
      "                    \"Data Engineering Excellence: Architecting Resilient Systems from Scratch to Scale (The Innovators of AI and Data Series Book 3)\" Book 3 of 3: The Innovators of AI and Data Series February 24, 2025    4.8  0.00\n",
      "                           \"Data Engineering with dbt: A practical guide to building a cloud-based, pragmatic, and dependable data platform with SQL\"                                     Roberto Zagni     June 30, 2023    4.7 37.00\n",
      "\n",
      "「✔」Data has been successfully saved to 'amazon_data_engineering_books.csv'.\n",
      "「✔」Data has been successfully inserted into MySQL!\n",
      "\n",
      "Query 1 executed in 0.0020 seconds.\n",
      "\n",
      "TOP-RATED DATA ENGINEERING BOOKS:\n",
      "\n",
      "                                                 Title  \\\n",
      "0    Fundamentals of Data Engineering: Plan and Bui...   \n",
      "1    Designing Data-Intensive Applications: The Big...   \n",
      "2    AI Engineering: Building Applications with Fou...   \n",
      "3    Financial Data Engineering: Design and Build D...   \n",
      "4    Data Pipelines Pocket Reference: Moving and Pr...   \n",
      "..                                                 ...   \n",
      "695  \"Data Engineering Design Patterns: Recipes for...   \n",
      "696  \"Hands-On Data Engineering with R, Python and ...   \n",
      "697  \"Data Engineering Design Patterns: Recipes for...   \n",
      "698  \"Hands-On Data Engineering with R, Python and ...   \n",
      "699  \"Data Engineering Design Patterns: Recipes for...   \n",
      "\n",
      "                     Author Rating  \n",
      "0    Joe Reis, Matt Housley    4.7  \n",
      "1          Martin Kleppmann    4.8  \n",
      "2                Chip Huyen    4.6  \n",
      "3            Tamer Khraisha    5.0  \n",
      "4            James Densmore    4.6  \n",
      "..                      ...    ...  \n",
      "695                 Unknown      0  \n",
      "696         Michel Ballings      0  \n",
      "697                 Unknown      0  \n",
      "698         Michel Ballings      0  \n",
      "699                 Unknown      0  \n",
      "\n",
      "[700 rows x 3 columns]\n",
      "\n",
      "BOOKS SORTED BY PRICE (Cheapest to Most Expensive):\n",
      "\n",
      "                                                 Title    Publication Date  \\\n",
      "0    Fundamentals of Data Engineering: A Comprehens...  September 22, 2024   \n",
      "1    Fundamentals of Data Engineering: A Comprehens...  September 22, 2024   \n",
      "2    \"Data Pipelines Pocket Reference: Moving and P...      March 16, 2021   \n",
      "3    \"Data Pipelines Pocket Reference: Moving and P...      March 16, 2021   \n",
      "4    \"Data Pipelines Pocket Reference: Moving and P...      March 16, 2021   \n",
      "..                                                 ...                 ...   \n",
      "475  Python Data Engineering Resources: Forge Your ...  September 27, 2024   \n",
      "476  \"The Data Engineering Handbook: We are Data En...    October 17, 2024   \n",
      "477  Mastering Data Profiling. Advanced Techniques ...   February 25, 2024   \n",
      "478  Mastering Data Profiling. Advanced Techniques ...   February 25, 2024   \n",
      "479  \"Mastering Data Profiling. Advanced Techniques...   February 25, 2024   \n",
      "\n",
      "    Price  \n",
      "0    14.0  \n",
      "1    14.0  \n",
      "2    17.0  \n",
      "3    17.0  \n",
      "4    17.0  \n",
      "..    ...  \n",
      "475   9.0  \n",
      "476   9.0  \n",
      "477  99.0  \n",
      "478  99.0  \n",
      "479  99.0  \n",
      "\n",
      "[480 rows x 3 columns]\n",
      "\n",
      "Query 2 executed in 0.0020 seconds.\n"
     ]
    }
   ],
   "source": [
    "import requests # sends http requests\n",
    "from bs4 import BeautifulSoup # used  for parsing html data\n",
    "import pandas as pd # handles data frames and data manipulation\n",
    "import time # used for sleep functions to delay scraping requests\n",
    "import random # generates random delays between requests\n",
    "import re # used for pattern matching\n",
    "import csv # writes data to .csv file\n",
    "import mysql.connector # used to connect to mysql database\n",
    "\n",
    "# TASK 1\n",
    "# AMAZON URL FOR DATA ENGINEERING BOOKS SEARCH RESULTS\n",
    "URL = \"https://www.amazon.com/s?k=data+engineering+books\"\n",
    "\n",
    "# USER-AGENT LIST TO AVOID DETECTION BY AMAZON\n",
    "USER_AGENTS = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.0.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.83 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36\"\n",
    "]\n",
    "\n",
    "# HEADERS TO MIMIC A REAL BROWSER REQUEST FOR SCRAPING\n",
    "HEADERS = {\n",
    "    \"User-Agent\": random.choice(USER_AGENTS),  # randomly choose a user-agent to avoid detection\n",
    "    \"Accept-Language\": \"en-US,en;q=0.5\"  # define acceptable languages in the request\n",
    "}\n",
    "\n",
    "# FUNCTION TO GET PUBLICATION DATE FROM BOOK'S PRODUCT PAGE\n",
    "def get_publication_date_from_product_page(book_url):\n",
    "    \"\"\"Fetch the publication date from a book's product page.\"\"\"\n",
    "    try:\n",
    "        # SEND REQUEST TO BOOK'S PRODUCT PAGE\n",
    "        product_response = requests.get(book_url, headers=HEADERS)\n",
    "        \n",
    "        # CHECK IF THE PAGE LOADS SUCCESSFULLY\n",
    "        if product_response.status_code == 200:\n",
    "            product_soup = BeautifulSoup(product_response.text, \"html.parser\")  # parses html response\n",
    "            # POSSIBLE LOCATIONS TO LOOK FOR PUBLICATION DATE\n",
    "            possible_locations = [\n",
    "                \"#detailBullets_feature_div\", \n",
    "                \"#productDetailsTable\", \n",
    "                \"#prodDetails\"\n",
    "            ]\n",
    "            for location in possible_locations:  # checks all locations\n",
    "                details = product_soup.select_one(location)  # gets the element containing details\n",
    "                if details:\n",
    "                    detail_text = details.get_text(strip=True)  # cleans up the text\n",
    "                    # SEARCH FOR A FULL DATE PATTERN\n",
    "                    date_match = re.search(r\"(January|February|March|April|May|June|July|August|September|October|November|December) \\d{1,2}, \\d{4}\", detail_text)\n",
    "                    if date_match:\n",
    "                        return date_match.group()  # returns the found date\n",
    "\n",
    "            # IF NO FULL DATE FOUND, SEARCH FOR A YEAR IN THE TEXT\n",
    "            year_match = re.search(r\"(19|20)\\d{2}\", product_soup.text)\n",
    "            if year_match:\n",
    "                return year_match.group()  # returns the year\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching product page: {e}\")  # prints error if exception error occurs\n",
    "    return None  # returns none if no date is found\n",
    "\n",
    "# FETCH MAIN SEARCH RESULTS PAGE FROM AMAZON\n",
    "response = requests.get(URL, headers=HEADERS)\n",
    "\n",
    "# CHECK IF PAGE RETRIEVAL WAS SUCCESSFUL\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")  # parses html of main page\n",
    "    titles, authors, pub_dates, ratings, prices = [], [], [], [], []  # lists to store scraped data\n",
    "    books = soup.find_all(\"div\", {\"data-component-type\": \"s-search-result\"})  # finds all book entries\n",
    "\n",
    "    # LOOP THROUGH FIRST 25 BOOKS\n",
    "    for book in books[:25]:\n",
    "        title_tag = book.find(\"h2\", class_=\"a-size-base-plus a-spacing-none a-color-base a-text-normal\")  # finds title\n",
    "        title = f'\"{title_tag.text.strip()}\"' if title_tag else None  # encloses title in quotes\n",
    "\n",
    "        # EXTRACT AUTHOR INFORMATION\n",
    "        author_tag = book.find(\"div\", class_=\"a-row a-size-base a-color-secondary\")\n",
    "        if author_tag:\n",
    "            author_links = author_tag.find_all(\"a\")\n",
    "            author = \", \".join([a.text.strip() for a in author_links]) if author_links else None  # combines author names\n",
    "        else:\n",
    "            author = None\n",
    "\n",
    "        # EXTRACT PUBLICATION DATE\n",
    "        pub_date = None\n",
    "        date_spans = book.find_all(\"span\", class_=\"a-size-base a-color-secondary\")  # finds all date spans\n",
    "        for span in date_spans:\n",
    "            span_text = span.get_text(strip=True)\n",
    "            # SEARCH FOR A FULL DATE FORMAT\n",
    "            date_match = re.search(r\"(January|February|March|April|May|June|July|August|September|October|November|December) \\d{1,2}, \\d{4}\", span_text)\n",
    "            if date_match:\n",
    "                pub_date = date_match.group()  # sets publication date\n",
    "                break\n",
    "        \n",
    "        # IF NO FULL DATE, SEARCH FOR A YEAR ONLY\n",
    "        if pub_date is None:\n",
    "            year_match = re.search(r\"(19|20)\\d{2}\", book.text)\n",
    "            if year_match:\n",
    "                pub_date = year_match.group()  # sets publication year\n",
    "\n",
    "        # EXTRACT RATING\n",
    "        rating_tag = book.find(\"span\", class_=\"a-icon-alt\")\n",
    "        rating = rating_tag.text.strip().split()[0] if rating_tag else None  # extracts rating value\n",
    "\n",
    "        # EXTRACT PRICE\n",
    "        price_tag = book.find(\"span\", class_=\"a-price-whole\")\n",
    "        if not price_tag:\n",
    "            price_tag = book.find(\"span\", class_=\"a-offscreen\")\n",
    "        \n",
    "        if price_tag:\n",
    "            price = price_tag.text.strip()\n",
    "            price = f\"{float(price):.2f}\" if price else None # displays 2 decimal places\n",
    "        else:\n",
    "            price = None\n",
    "\n",
    "        # EXTRACT BOOK URL FOR FURTHER DETAILS\n",
    "        book_url_tag = book.find(\"a\", class_=\"a-link-normal s-no-outline\")\n",
    "        book_url = \"https://www.amazon.com\" + book_url_tag[\"href\"] if book_url_tag else None  # creates full url\n",
    "\n",
    "        # FETCH PUBLICATION DATE FROM PRODUCT PAGE IF MISSING\n",
    "        if pub_date is None and book_url:\n",
    "            pub_date = get_publication_date_from_product_page(book_url)\n",
    "\n",
    "        # APPEND EXTRACTED DATA TO LISTS\n",
    "        titles.append(title)\n",
    "        authors.append(author)\n",
    "        pub_dates.append(pub_date)\n",
    "        ratings.append(rating)\n",
    "        prices.append(price)\n",
    "\n",
    "        # RESPECT AMAZON SCRAPING GUIDELINES BY ADDING RANDOM DELAY\n",
    "        time.sleep(random.uniform(1, 3))\n",
    "\n",
    "    # CREATE PANDAS DATAFRAME WITH SCRAPED DATA\n",
    "    df = pd.DataFrame({\n",
    "        \"Title\": titles,\n",
    "        \"Author\": authors,\n",
    "        \"Publication Date\": pub_dates,\n",
    "        \"Rating\": ratings,\n",
    "        \"Price\": prices\n",
    "    })\n",
    "\n",
    "    \n",
    "    # SAVE DATA TO CSV WITH PROPER QUOTING\n",
    "    df.to_csv(\"amazon_data_engineering_books.csv\", index=False, quoting=csv.QUOTE_ALL)\n",
    "\n",
    "    # DISPLAY THE SCRAPED DATA\n",
    "    print(\"\\nScraped Data:\\n\")\n",
    "    print(df.to_string(index=False))\n",
    "    print(\"\\n「✔」Data has been successfully saved to 'amazon_data_engineering_books.csv'.\")\n",
    "else:\n",
    "    print(\"  ⓘ Failed to retrieve webpage. Amazon may have blocked the request.\")  # error handling failure in retrieval\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# TASK 2\n",
    "# ENSURE DATABASE CONNECTION AND CREATION\n",
    "def create_database():\n",
    "    \"\"\"Ensures that the MySQL database exists.\"\"\"\n",
    "    # CREATE CONNECTION TO MYSQL SERVER\n",
    "    conn = mysql.connector.connect(\n",
    "        host=\"localhost\",\n",
    "        user=\"root\",\n",
    "        password=\"353txRQ8\"\n",
    "    )\n",
    "    # CREATE CURSOR OBJECT TO INTERACT WITH THE DATABASE\n",
    "    cursor = conn.cursor()\n",
    "    # CREATE DATABASE IF IT DOESN'T EXIST\n",
    "    cursor.execute(\"CREATE DATABASE IF NOT EXISTS data_engineering_books\")\n",
    "    # COMMIT THE TRANSACTION\n",
    "    conn.commit()\n",
    "    # CLOSE CURSOR AND CONNECTION\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "# ENSURE TABLE CREATION\n",
    "def create_table():\n",
    "    \"\"\"Ensures that the 'books' table exists with the correct schema.\"\"\"\n",
    "    # CREATE CONNECTION TO MYSQL SERVER WITH DATABASE SELECTION\n",
    "    conn = mysql.connector.connect(\n",
    "        host=\"localhost\",\n",
    "        user=\"root\",\n",
    "        password=\"353txRQ8\",\n",
    "        database=\"data_engineering_books\"\n",
    "    )\n",
    "    # CREATE CURSOR OBJECT TO INTERACT WITH THE DATABASE\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # CREATE TABLE IF IT DOESN'T EXIST\n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS books (\n",
    "            id INT AUTO_INCREMENT PRIMARY KEY,  # PRIMARY KEY AND AUTO INCREMENT FOR ID COLUMN\n",
    "            title VARCHAR(255),  # BOOK TITLE COLUMN\n",
    "            author VARCHAR(255),  # BOOK AUTHOR COLUMN\n",
    "            publication_date VARCHAR(50),  # PUBLICATION DATE COLUMN\n",
    "            rating VARCHAR(10),  # BOOK RATING COLUMN\n",
    "            price VARCHAR(50)  # BOOK PRICE COLUMN\n",
    "        )\n",
    "    \"\"\")\n",
    "    \n",
    "    # COMMIT THE TRANSACTION TO SAVE CHANGES\n",
    "    conn.commit()\n",
    "    # CLOSE CURSOR AND CONNECTION\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "# LOAD DATA FROM THE CSV FILE AND HANDLE NaN VALUES\n",
    "def load_data():\n",
    "    \"\"\"Loads scraped data from CSV, handles missing values, and inserts into MySQL.\"\"\"\n",
    "    # READ DATA FROM CSV FILE INTO A PANDAS DATAFRAME\n",
    "    df = pd.read_csv(\"amazon_data_engineering_books.csv\")\n",
    "\n",
    "    # FILL NaN VALUES WITH DEFAULT TEXT FOR TEXT FIELDS AND \"NOT AVAILABLE\" FOR NUMERIC FIELDS\n",
    "    df.fillna({\n",
    "        'Title': 'Unknown',  # fills empty values in 'Title' with 'Unknown'\n",
    "        'Author': 'Unknown', \n",
    "        'Publication Date': 'Not Available',\n",
    "        'Rating': '0', \n",
    "        'Price': 'Not Available'  \n",
    "    }, inplace=True)\n",
    "\n",
    "    conn = None\n",
    "    cursor = None\n",
    "\n",
    "    try:\n",
    "        # CREATE CONNECTION TO MYSQL DATABASE\n",
    "        conn = mysql.connector.connect(\n",
    "            host=\"localhost\",\n",
    "            user=\"root\",\n",
    "            password=\"353txRQ8\",\n",
    "            database=\"data_engineering_books\"\n",
    "        )\n",
    "        # CREATE CURSOR OBJECT TO INTERACT WITH THE DATABASE\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # INSERT DATA INTO THE 'books' TABLE\n",
    "        for _, row in df.iterrows():\n",
    "            cursor.execute(\"\"\"\n",
    "                INSERT INTO books (title, author, publication_date, rating, price)\n",
    "                VALUES (%s, %s, %s, %s, %s)\n",
    "            \"\"\", (row['Title'], row['Author'], row['Publication Date'], row['Rating'], row['Price']))\n",
    "        \n",
    "        # COMMIT THE TRANSACTION TO SAVE CHANGES\n",
    "        conn.commit()\n",
    "        print(\"「✔」Data has been successfully inserted into MySQL!\")\n",
    "\n",
    "    except mysql.connector.Error as db_error:\n",
    "        # PRINT ERROR MESSAGE IN CASE OF A DATABASE ERROR\n",
    "        print(f\"Database error: {db_error}\")\n",
    "\n",
    "    finally:\n",
    "        # CLOSE CURSOR AND CONNECTION AFTER USE\n",
    "        if cursor:\n",
    "            cursor.close()\n",
    "        if conn:\n",
    "            conn.close()\n",
    "\n",
    "# EXTRACT AND SORT DATA\n",
    "def query_books():\n",
    "    \"\"\"Executes the SQL query to extract and sort books based on rating.\"\"\"\n",
    "    \n",
    "    # CREATE SEPARATE CONNECTION FOR QUERY1\n",
    "    conn1 = mysql.connector.connect(\n",
    "        host=\"localhost\",\n",
    "        user=\"root\",\n",
    "        password=\"353txRQ8\",\n",
    "        database=\"data_engineering_books\"\n",
    "    )\n",
    "    cursor1 = conn1.cursor()\n",
    "\n",
    "    # MEASURE SQL QUERY1 EXECUTION TIME - START TIME\n",
    "    start_time1 = time.time()\n",
    "\n",
    "    # SELECT TITLE, AUTHOR, AND RATING COLUMNS, ORDERING BY RATING IN DESCENDING ORDER\n",
    "    query1 = \"\"\"\n",
    "        SELECT title, author, rating\n",
    "        FROM books\n",
    "        ORDER BY \n",
    "            CASE \n",
    "                WHEN rating = 'No rating' THEN 0\n",
    "                ELSE CAST(rating AS DECIMAL)\n",
    "            END DESC;\n",
    "\n",
    "    \"\"\"\n",
    "    cursor1.execute(query1)\n",
    "    results1 = cursor1.fetchall()\n",
    "\n",
    "    # END TIME\n",
    "    end_time1 = time.time()\n",
    "    execution_time1 = end_time1 - start_time1  # calculates query execution time\n",
    "    print(f\"\\nQuery 1 executed in {execution_time1:.4f} seconds.\") # displays execution time\n",
    "\n",
    "    # CREATE A PANDAS DATAFRAME FROM THE FIRST QUERY RESULT\n",
    "    df_results1 = pd.DataFrame(results1, columns=[\"Title\", \"Author\", \"Rating\"])\n",
    "    print(\"\\nTOP-RATED DATA ENGINEERING BOOKS:\\n\")\n",
    "    print(df_results1)\n",
    "\n",
    "    # CLOSE CURSOR AND CONNECTION FOR THE FIRST QUERY\n",
    "    cursor1.close()\n",
    "    conn1.close()\n",
    "\n",
    "    \n",
    "    # CREATE SEPARATE CONNECTION FOR QUERY2\n",
    "    conn2 = mysql.connector.connect(\n",
    "        host=\"localhost\",\n",
    "        user=\"root\",\n",
    "        password=\"353txRQ8\",\n",
    "        database=\"data_engineering_books\"\n",
    "    )\n",
    "    cursor2 = conn2.cursor()\n",
    "\n",
    "    # MEASURE SQL QUERY2 EXECUTION TIME - START TIME\n",
    "    start_time2 = time.time()\n",
    "    \n",
    "    # SELECT TITLE, PUBLICATION DATE, AND PRICE COLUMNS, ORDERING BY PUBLICATION DATE IN ASCENDING ORDER\n",
    "    query2 = \"\"\"\n",
    "        SELECT title, publication_date, price\n",
    "        FROM books\n",
    "        WHERE price > 0\n",
    "        ORDER BY price ASC;\n",
    "    \"\"\"\n",
    "    cursor2.execute(query2)\n",
    "    results2 = cursor2.fetchall()\n",
    "\n",
    "    # END TIME\n",
    "    end_time2 = time.time()\n",
    "    execution_time2 = end_time2 - start_time2  ## calculates query execution time\n",
    "    print(f\"\\nQuery 2 executed in {execution_time2:.4f} seconds.\") # displays execution time\n",
    "\n",
    "    # CREATE A PANDAS DATAFRAME FROM THE SECOND QUERY RESULT\n",
    "    df_results2 = pd.DataFrame(results2, columns=[\"Title\", \"Publication Date\", \"Price\"])\n",
    "    print(\"\\nBOOKS SORTED BY PRICE (Cheapest to Most Expensive):\\n\")\n",
    "    print(df_results2)\n",
    "\n",
    "    # CLOSE CURSOR AND CONNECTION FOR THE SECOND QUERY\n",
    "    cursor2.close()\n",
    "    conn2.close()\n",
    "\n",
    "# EXECUTE FUNCTIONS\n",
    "create_database()  # creates database if it doesn't exist\n",
    "create_table()  # creates'books' table if it doesn't exist\n",
    "load_data()  # loads data into database from .csv file\n",
    "query_books()  # calls queries and displays results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20198f49-6759-406b-b9cf-cd6bc2fe1260",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
