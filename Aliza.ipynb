import requests  # Library for making HTTP requests
from bs4 import BeautifulSoup  # Library for parsing HTML content
import pandas as pd  # Library for handling data in tabular format
import time  # Library for adding delays in execution
import random  # Library for generating random numbers
import re  # Library for working with regular expressions
import csv  # Library for working with CSV files

# Amazon URL for searching Data Engineering books
URL = "https://www.amazon.com/s?k=data+engineering+books"

# List of different User-Agent strings to avoid detection
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.83 Safari/537.36",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36"
]

# HTTP headers to simulate a real browser request
HEADERS = {
    "User-Agent": random.choice(USER_AGENTS),  # Randomly selects a User-Agent to reduce detection risk
    "Accept-Language": "en-US,en;q=0.5"  # Requests content in English
}

def fetch_page(url, headers, max_retries=3):
    """Fetch the webpage content with retries in case of temporary blocks."""
    for attempt in range(max_retries):  # Retry mechanism for handling temporary failures
        try:
            response = requests.get(url, headers=headers)  # Send HTTP request to the URL
            if response.status_code == 200:  # If successful response
                return response.text  # Return the HTML content
            elif response.status_code == 503:  # If Amazon blocks the request (Service Unavailable)
                print(f"Retrying... Attempt {attempt+1} of {max_retries}")
                time.sleep(random.uniform(5, 10))  # Wait before retrying
            else:
                print(f"Request failed with status code {response.status_code}")
                return None  # Exit if an unknown status code is encountered
        except Exception as e:
            print(f"Error fetching page: {e}")  # Handle exceptions gracefully
    return None  # Return None if all retries fail

# Fetch the main search results page
html_content = fetch_page(URL, HEADERS)

if html_content:
    # Parse the HTML content using BeautifulSoup
    soup = BeautifulSoup(html_content, "html.parser")
    
    # Lists to store extracted book data
    titles, authors, pub_dates, ratings, prices = [], [], [], [], []
    
    # Find all book elements in the search results
    books = soup.find_all("div", {"data-component-type": "s-search-result"})
    
    # Loop through the first 15 books to extract details
    for book in books[:15]:  # Limit to 15 results to avoid excessive requests
        try:
            # Extract book title
            title_tag = book.find("h2", class_="a-size-base-plus")  # Locate title tag
            if title_tag:
                title = title_tag.get_text(strip=True)  # Get text content of title
            else:
                title = "Unknown Title"  # Default value if title is missing
            
            # Extract author information
            author_tag = book.find("div", class_="a-row a-size-base a-color-secondary")  # Locate author section
            if author_tag:
                author_links = author_tag.find_all("a")  # Find all author links
                author = ", ".join([a.text.strip() for a in author_links]) if author_links else author_tag.text.strip()
            else:
                author = "Unknown Author"  # Default value if author is missing
            
            # Extract publication date
            pub_date = None  # Initialize publication date
            date_spans = book.find_all("span", class_="a-size-base a-color-secondary")  # Locate all potential date spans
            for span in date_spans:
                date_match = re.search(r"(January|February|March|April|May|June|July|August|September|October|November|December) \d{1,2}, \d{4}", span.text)
                if date_match:
                    pub_date = date_match.group()  # Extract the first matched date
                    break  # Stop checking after finding a valid date
            
            # Extract rating
            rating_tag = book.find("span", class_="a-icon-alt")  # Locate rating tag
            rating = rating_tag.text.strip().split()[0] if rating_tag else "No Rating"  # Extract rating if available
            
            # Extract price (prioritizing hardcover/paperback over Kindle)
            price_tag = book.find("span", class_="a-price-whole")  # Locate price tag
            if not price_tag:
                price_tag = book.find("span", class_="a-offscreen")  # Fallback price option
            price = price_tag.text.strip().replace("$", "").replace(",", "") if price_tag else "Not Available"  # Clean price
            
            # Append extracted data to lists
            titles.append(title)
            authors.append(author)
            pub_dates.append(pub_date)
            ratings.append(rating)
            prices.append(price)
            
            # Respect Amazon scraping guidelines by adding a delay
            time.sleep(random.uniform(2, 4))  # Sleep for a random time to mimic human behavior
        
        except Exception as e:
            print(f"Error processing book: {e}")  # Handle any extraction errors
    
    # Create a DataFrame from the extracted data
    df = pd.DataFrame({
        "Title": titles,
        "Author": authors,
        "Publication_Date": pub_dates,
        "Rating": ratings,
        "Price": prices
    })
    
    # Save the extracted data to a CSV file with UTF-8 encoding
    df.to_csv("amazon_books_data.csv", index=False, encoding="utf-8", sep=",", quoting=csv.QUOTE_MINIMAL)
    
    print("\n✅ Data successfully saved to 'amazon_books_data.csv'.")  # Confirmation message
else:
    print("❌ Failed to retrieve webpage. Amazon may have blocked the request.")  # Error message if request fails
