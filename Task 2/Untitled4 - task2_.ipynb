{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14431f3d-3e81-49e9-b217-01e050ae0bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mysql-connector-python\n",
      "  Downloading mysql_connector_python-9.2.0-cp312-cp312-macosx_14_0_arm64.whl.metadata (6.0 kB)\n",
      "Downloading mysql_connector_python-9.2.0-cp312-cp312-macosx_14_0_arm64.whl (15.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.1/15.1 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: mysql-connector-python\n",
      "Successfully installed mysql-connector-python-9.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install mysql-connector-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "821fa59b-b69f-4f54-98b9-5dd5e3a01448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been saved to 'data_engineering_books.csv'\n",
      "database error: 1049 (42000): Unknown database 'data_engineering_books'\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cursor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 149\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatabase error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdb_error\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m--> 149\u001b[0m     cursor\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;66;03m# closes connection to database\u001b[39;00m\n\u001b[1;32m    151\u001b[0m     conn\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cursor' is not defined"
     ]
    }
   ],
   "source": [
    "import requests  # makes http requests\n",
    "from bs4 import BeautifulSoup  # parses html content\n",
    "import pandas as pd  # data manipulation and storage\n",
    "import time  # delays between requests\n",
    "import random  # adds variability in request delays\n",
    "import re  # pattern matching in text\n",
    "import mysql.connector\n",
    "\n",
    "# amazon url for data engineering books\n",
    "URL = \"https://www.packtpub.com/en-us/search?q=data%20engineering%20books&country=us&language=en\"\n",
    "\n",
    "# user-agent list to avoid detection by amazon's anti-scraping measures\n",
    "USER_AGENTS = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.0.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.83 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36\"\n",
    "]\n",
    "\n",
    "# headers to mimic a real browser request\n",
    "HEADERS = {\n",
    "    \"User-Agent\": random.choice(USER_AGENTS),  # randomly selects a user-agent to reduce detection risk\n",
    "    \"Accept-Language\": \"en-US,en;q=0.5\"  # specifies the preferred language for the response\n",
    "}\n",
    "\n",
    "def get_publication_date_from_product_page(book_url):\n",
    "    \"\"\"fetches the publication date from a book's product page.\"\"\"\n",
    "    try:\n",
    "        product_response = requests.get(book_url, headers=HEADERS)  # sends request to the book's product page\n",
    "        if product_response.status_code == 200:  # checks if request was successful\n",
    "            product_soup = BeautifulSoup(product_response.text, \"html.parser\")  # parses html content\n",
    "            possible_locations = [  # defines possible locations where publication date might be found\n",
    "                \"#detailBullets_feature_div\",\n",
    "                \"#productDetailsTable\",\n",
    "                \"#prodDetails\"\n",
    "            ]\n",
    "            for location in possible_locations:\n",
    "                details = product_soup.select_one(location)  # selects the first matching html element\n",
    "                if details:\n",
    "                    detail_text = details.get_text(strip=True)  # extracts text and removes extra spaces\n",
    "                    date_match = re.search(r\"(January|February|March|April|May|June|July|August|September|October|November|December) \\d{1,2}, \\d{4}\", detail_text)\n",
    "                    if date_match:\n",
    "                        return date_match.group()  # returns the matched publication date\n",
    "\n",
    "            year_match = re.search(r\"(19|20)\\d{2}\", product_soup.text)  # searches for a 4-digit year in the text\n",
    "            if year_match:\n",
    "                return year_match.group()  # returns the matched year\n",
    "    except Exception as e:\n",
    "        print(f\"error fetching product page: {e}\")  # prints error message if fetching fails\n",
    "    return None  # returns none if no publication date is found\n",
    "\n",
    "response = requests.get(URL, headers=HEADERS)  # sends a request to amazon\n",
    "\n",
    "if response.status_code == 200:  # checks if request was successful\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")  # parses html content\n",
    "    books = soup.find_all(\"div\", {\"data-component-type\": \"s-search-result\"})  # finds all book listings\n",
    "\n",
    "    # lists to store extracted book data\n",
    "    titles, authors, pub_dates, ratings, prices = [], [], [], [], []\n",
    "\n",
    "    for book in books[:50]:  # loops through the first 50 book listings\n",
    "        title_tag = book.find(\"h2\", class_=\"a-size-base-plus a-spacing-none a-color-base a-text-normal\")  # finds book title\n",
    "        title = title_tag.text.strip() if title_tag else \"unknown\"  # extracts title text or assigns \"unknown\"\n",
    "\n",
    "        author_tag = book.find(\"div\", class_=\"a-row a-size-base a-color-secondary\")  # finds author information\n",
    "        author_links = author_tag.find_all(\"a\") if author_tag else []  # gets all author links if present\n",
    "        author = \", \".join([a.text.strip() for a in author_links]) if author_links else \"unknown\"  # extracts author names\n",
    "\n",
    "        pub_date = None  # initializes publication date variable\n",
    "        date_spans = book.find_all(\"span\", class_=\"a-size-base a-color-secondary\")  # finds potential publication date elements\n",
    "        for span in date_spans:\n",
    "            span_text = span.get_text(strip=True)  # extracts text content\n",
    "            date_match = re.search(r\"(January|February|March|April|May|June|July|August|September|October|November|December) \\d{1,2}, \\d{4}\", span_text)\n",
    "            if date_match:\n",
    "                pub_date = date_match.group()  # assigns found publication date\n",
    "                break\n",
    "\n",
    "        if pub_date is None:\n",
    "            year_match = re.search(r\"(19|20)\\d{2}\", book.text)  # searches for a 4-digit year\n",
    "            if year_match:\n",
    "                pub_date = year_match.group()  # assigns found year\n",
    "\n",
    "        rating_tag = book.find(\"span\", class_=\"a-icon-alt\")  # finds book rating\n",
    "        rating = rating_tag.text.strip().split()[0] if rating_tag else \"no rating\"  # extracts rating value\n",
    "\n",
    "        price_tag = book.find(\"span\", class_=\"a-price-whole\") or book.find(\"span\", class_=\"a-offscreen\")  # finds price\n",
    "        price = price_tag.text.strip() if price_tag else \"not available\"  # extracts price or assigns \"not available\"\n",
    "\n",
    "        book_url_tag = book.find(\"a\", class_=\"a-link-normal s-no-outline\")  # finds book url\n",
    "        book_url = \"https://www.amazon.com\" + book_url_tag[\"href\"] if book_url_tag else None  # constructs full url\n",
    "\n",
    "        if pub_date is None and book_url:\n",
    "            pub_date = get_publication_date_from_product_page(book_url)  # gets publication date from product page if missing\n",
    "\n",
    "        # appends extracted data to lists\n",
    "        titles.append(title)\n",
    "        authors.append(author)\n",
    "        pub_dates.append(pub_date)\n",
    "        ratings.append(rating)\n",
    "        prices.append(price)\n",
    "        time.sleep(random.uniform(1, 3))  # adds random delay to avoid detection\n",
    "\n",
    "    # creates a pandas dataframe with collected book data\n",
    "    df = pd.DataFrame({\n",
    "        \"Title\": titles,\n",
    "        \"Author\": authors,\n",
    "        \"Publication Date\": pub_dates,\n",
    "        \"Rating\": ratings,\n",
    "        \"Price\": prices\n",
    "    })\n",
    "\n",
    "    # saves data to csv file\n",
    "    df.to_csv(\"amazon_data_engineering_books1.csv\", index=False)\n",
    "    print(\"Data has been saved to 'data_engineering_books.csv'\")\n",
    "\n",
    "    try:\n",
    "        # establishes database connection\n",
    "        conn = mysql.connector.connect(\n",
    "            host=\"localhost\",\n",
    "            user=\"root\",\n",
    "            password=\"Aliza.123\",\n",
    "            database=\"data_engineering_books\"\n",
    "        )\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        for index, row in df.iterrows():  # iterates through dataframe rows\n",
    "            if all(pd.notnull(val) and val != \"unknown\" for val in row):  # checks if row data is valid\n",
    "                cursor.execute(\"\"\"\n",
    "                    INSERT INTO books (title, author, publication_date, rating, price)\n",
    "                    VALUES (%s, %s, %s, %s, %s)\n",
    "                \"\"\", (row['Title'], row['Author'], row['Publication Date'], row['Rating'], row['Price']))  # inserts data\n",
    "        conn.commit()  # commits transaction\n",
    "\n",
    "        # query that extract only 3 columns from the table and sorts table based on rating column\n",
    "        query = \"\"\"\n",
    "            SELECT title, author, price\n",
    "            FROM books\n",
    "            ORDER BY rating ASC;\n",
    "        \"\"\"\n",
    "        cursor.execute(query)\n",
    "        results = cursor.fetchall()\n",
    "        df_result = pd.DataFrame(results, columns=[\"Title\", \"Author\", \"Price\", \"Rating\"])\n",
    "        print(df_result)\n",
    "\n",
    "    # handles database errors\n",
    "    except mysql.connector.Error as db_error:\n",
    "        print(f\"database error: {db_error}\")\n",
    "\n",
    "    finally:\n",
    "        cursor.close()\n",
    "        # closes connection to database\n",
    "        conn.close()\n",
    "        print(\"\\nThese are the top-rated data engineering books our shoppers love ♡. \\nHappy reads!\")\n",
    "\n",
    "else:\n",
    "    # error handling message for user if parsing fails\n",
    "    print(\"ⓘ Failed to retrieve webpage. Amazon may have blocked the request. :(\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1b845a9-6153-40d3-8bdc-58e6537b1a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database error: 1054 (42S22): Unknown column 'nan' in 'field list'\n",
      "\n",
      "Top-rated Data Engineering Books:\n",
      "\n",
      "Empty DataFrame\n",
      "Columns: [Title, Author, Price, Publication Date]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import mysql.connector\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure database connection and creation\n",
    "def create_database():\n",
    "    \"\"\"Ensures that the MySQL database exists.\"\"\"\n",
    "    conn = mysql.connector.connect(\n",
    "        host=\"localhost\",\n",
    "        user=\"root\",\n",
    "        password=\"Aliza.123\"\n",
    "    )\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"CREATE DATABASE IF NOT EXISTS data_engineering_books\")\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "# Ensure table creation\n",
    "def create_table():\n",
    "    \"\"\"Ensures that the 'books' table exists with the correct schema.\"\"\"\n",
    "    conn = mysql.connector.connect(\n",
    "        host=\"localhost\",\n",
    "        user=\"root\",\n",
    "        password=\"Aliza.123\",\n",
    "        database=\"data_engineering_books\"\n",
    "    )\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS books (\n",
    "            id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "            title VARCHAR(255),\n",
    "            author VARCHAR(255),\n",
    "            publication_date VARCHAR(50),\n",
    "            rating VARCHAR(10),\n",
    "            price VARCHAR(50)\n",
    "        )\n",
    "    \"\"\")\n",
    "    \n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "# Load data from the CSV file\n",
    "def load_data():\n",
    "    \"\"\"Loads scraped data from CSV and inserts it into MySQL.\"\"\"\n",
    "    df = pd.read_csv(\"amazon_data_engineering_books.csv\")\n",
    "\n",
    "    conn = None\n",
    "    cursor = None\n",
    "\n",
    "    try:\n",
    "        conn = mysql.connector.connect(\n",
    "            host=\"localhost\",\n",
    "            user=\"root\",\n",
    "            password=\"Aliza.123\",\n",
    "            database=\"data_engineering_books\"\n",
    "        )\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Insert data into MySQL\n",
    "        for _, row in df.iterrows():\n",
    "            cursor.execute(\"\"\"\n",
    "                INSERT INTO books (title, author, publication_date, rating, price)\n",
    "                VALUES (%s, %s, %s, %s, %s)\n",
    "            \"\"\", (row['Title'], row['Author'], row['Publication Date'], row['Rating'], row['Price']))\n",
    "        \n",
    "        conn.commit()\n",
    "        print(\"Data successfully inserted into MySQL!\")\n",
    "\n",
    "    except mysql.connector.Error as db_error:\n",
    "        print(f\"Database error: {db_error}\")\n",
    "\n",
    "    finally:\n",
    "        if cursor:\n",
    "            cursor.close()\n",
    "        if conn:\n",
    "            conn.close()\n",
    "\n",
    "# Extract and sort data\n",
    "def query_books():\n",
    "    \"\"\"Executes the SQL query to extract and sort books based on rating.\"\"\"\n",
    "    conn = mysql.connector.connect(\n",
    "        host=\"localhost\",\n",
    "        user=\"root\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2720e265-39d1-460b-b374-1558e8800660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully inserted into MySQL!\n",
      "\n",
      "Top-rated Data Engineering Books:\n",
      "\n",
      "                                                Title  \\\n",
      "0   \"Cracking the Data Engineering Interview: Land...   \n",
      "1   \"Cracking the Data Engineering Interview: Land...   \n",
      "2   \"Data Engineering with Python: Work with massi...   \n",
      "3   \"Data Engineering with AWS: Acquire the skills...   \n",
      "4   \"Data Engineering with AWS: Acquire the skills...   \n",
      "5   \"Data Engineering with Alteryx: Helping data e...   \n",
      "6   \"Data Engineering with Databricks Cookbook: Bu...   \n",
      "7                         \"Data Engineering on Azure\"   \n",
      "8   \"AI Engineering: Building Applications with Fo...   \n",
      "9   \"Data Pipelines Pocket Reference: Moving and P...   \n",
      "10  \"Storytelling with Data: A Data Visualization ...   \n",
      "11  \"Data Labeling in Machine Learning with Python...   \n",
      "12  \"Fundamentals of Data Engineering: Plan and Bu...   \n",
      "13  \"Building Data Science Applications with FastA...   \n",
      "14                       \"Snowflake Data Engineering\"   \n",
      "15  \"Data Engineering Excellence: Architecting Res...   \n",
      "16  \"Data Engineering with dbt: A practical guide ...   \n",
      "17  \"Designing Data-Intensive Applications: The Bi...   \n",
      "18  \"Data-Driven Science and Engineering: Machine ...   \n",
      "19  \"Python Data Engineering Resources: Forge Your...   \n",
      "20  \"Financial Data Engineering: Design and Build ...   \n",
      "21  \"Data Engineering Best Practices: Architect ro...   \n",
      "22  \"The Data Engineering Handbook: We are Data En...   \n",
      "23  \"Data Engineering with AWS Cookbook: A recipe-...   \n",
      "24  \"Data Engineering Design Patterns: Recipes for...   \n",
      "\n",
      "                                               Author Price  \\\n",
      "0                                      Kedeisha Bryan  26.0   \n",
      "1                                      Kedeisha Bryan  26.0   \n",
      "2                                       Paul Crickard  37.0   \n",
      "3                                        Gareth Eagar  24.0   \n",
      "4                                        Gareth Eagar  24.0   \n",
      "5                                       Paul Houghton  31.0   \n",
      "6                                       Pulkit Chadha  39.0   \n",
      "7                                       Vlad Riscutia  43.0   \n",
      "8                                          Chip Huyen  68.0   \n",
      "9                                      James Densmore  17.0   \n",
      "10                            Cole Nussbaumer Knaflic  21.0   \n",
      "11                                  Vijaya Kumar Suda  44.0   \n",
      "12                             Joe Reis, Matt Housley  43.0   \n",
      "13                                     François Voron  47.0   \n",
      "14                                         Maja Ferle  49.0   \n",
      "15  Book 3 of 3: The Innovators of AI and Data Series   0.0   \n",
      "16                                      Roberto Zagni  37.0   \n",
      "17                                   Martin Kleppmann  47.0   \n",
      "18                  Steven L. Brunton, J. Nathan Kutz  60.0   \n",
      "19  Book 1 of 4: Future-Proof Tech Skills: Includi...   9.0   \n",
      "20                                     Tamer Khraisha  60.0   \n",
      "21              Richard J. Schiller, David Larochelle  39.0   \n",
      "22                                            Unknown   9.0   \n",
      "23          Trâm Ngọc Phạm, Gonzalo Herreros González  49.0   \n",
      "24                                            Unknown  79.0   \n",
      "\n",
      "      Publication Date  \n",
      "0     November 7, 2023  \n",
      "1     November 7, 2023  \n",
      "2     October 23, 2020  \n",
      "3     October 31, 2023  \n",
      "4     October 31, 2023  \n",
      "5        June 30, 2022  \n",
      "6         May 31, 2024  \n",
      "7      August 17, 2021  \n",
      "8      January 7, 2025  \n",
      "9       March 16, 2021  \n",
      "10    November 2, 2015  \n",
      "11    February 9, 2024  \n",
      "12       July 26, 2022  \n",
      "13       July 31, 2023  \n",
      "14    January 28, 2025  \n",
      "15   February 24, 2025  \n",
      "16       June 30, 2023  \n",
      "17       April 2, 2017  \n",
      "18       July 28, 2022  \n",
      "19  September 27, 2024  \n",
      "20   November 12, 2024  \n",
      "21    October 11, 2024  \n",
      "22    October 17, 2024  \n",
      "23   November 29, 2024  \n",
      "24        June 3, 2025  \n"
     ]
    }
   ],
   "source": [
    "import mysql.connector\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure database connection and creation\n",
    "def create_database():\n",
    "    \"\"\"Ensures that the MySQL database exists.\"\"\"\n",
    "    conn = mysql.connector.connect(\n",
    "        host=\"localhost\",\n",
    "        user=\"root\",\n",
    "        password=\"Aliza.123\"\n",
    "    )\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"CREATE DATABASE IF NOT EXISTS data_engineering_books\")\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "# Ensure table creation\n",
    "def create_table():\n",
    "    \"\"\"Ensures that the 'books' table exists with the correct schema.\"\"\"\n",
    "    conn = mysql.connector.connect(\n",
    "        host=\"localhost\",\n",
    "        user=\"root\",\n",
    "        password=\"Aliza.123\",\n",
    "        database=\"data_engineering_books\"\n",
    "    )\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS books (\n",
    "            id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "            title VARCHAR(255),\n",
    "            author VARCHAR(255),\n",
    "            publication_date VARCHAR(50),\n",
    "            rating VARCHAR(10),\n",
    "            price VARCHAR(50)\n",
    "        )\n",
    "    \"\"\")\n",
    "    \n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "# Load data from the CSV file and handle NaN values\n",
    "def load_data():\n",
    "    \"\"\"Loads scraped data from CSV, handles missing values, and inserts into MySQL.\"\"\"\n",
    "    df = pd.read_csv(\"amazon_data_engineering_books.csv\")\n",
    "\n",
    "    # Fill NaN values with \"Unknown\" for text fields, \"Not Available\" for price/publication date\n",
    "    df.fillna({\n",
    "        'Title': 'Unknown',\n",
    "        'Author': 'Unknown',\n",
    "        'Publication Date': 'Not Available',\n",
    "        'Rating': 'No Rating',\n",
    "        'Price': 'Not Available'\n",
    "    }, inplace=True)\n",
    "\n",
    "    conn = None\n",
    "    cursor = None\n",
    "\n",
    "    try:\n",
    "        conn = mysql.connector.connect(\n",
    "            host=\"localhost\",\n",
    "            user=\"root\",\n",
    "            password=\"Aliza.123\",\n",
    "            database=\"data_engineering_books\"\n",
    "        )\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Insert data into MySQL\n",
    "        for _, row in df.iterrows():\n",
    "            cursor.execute(\"\"\"\n",
    "                INSERT INTO books (title, author, publication_date, rating, price)\n",
    "                VALUES (%s, %s, %s, %s, %s)\n",
    "            \"\"\", (row['Title'], row['Author'], row['Publication Date'], row['Rating'], row['Price']))\n",
    "        \n",
    "        conn.commit()\n",
    "        print(\"Data successfully inserted into MySQL!\")\n",
    "\n",
    "    except mysql.connector.Error as db_error:\n",
    "        print(f\"Database error: {db_error}\")\n",
    "\n",
    "    finally:\n",
    "        if cursor:\n",
    "            cursor.close()\n",
    "        if conn:\n",
    "            conn.close()\n",
    "\n",
    "# Extract and sort data\n",
    "def query_books():\n",
    "    \"\"\"Executes the SQL query to extract and sort books based on rating.\"\"\"\n",
    "    conn = mysql.connector.connect(\n",
    "        host=\"localhost\",\n",
    "        user=\"root\",\n",
    "        password=\"Aliza.123\",\n",
    "        database=\"data_engineering_books\"\n",
    "    )\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    query = \"\"\"\n",
    "        SELECT title, author, price, publication_date\n",
    "        FROM books\n",
    "        ORDER BY rating ASC;\n",
    "    \"\"\"\n",
    "    cursor.execute(query)\n",
    "    results = cursor.fetchall()\n",
    "\n",
    "    df_result = pd.DataFrame(results, columns=[\"Title\", \"Author\", \"Price\", \"Publication Date\"])\n",
    "    \n",
    "    print(\"\\nTop-rated Data Engineering Books:\\n\")\n",
    "    print(df_result)\n",
    "\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "# Execute functions\n",
    "create_database()\n",
    "create_table()\n",
    "load_data()\n",
    "query_books()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9013f3-64bc-499d-9f57-0e648c7ab6d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
