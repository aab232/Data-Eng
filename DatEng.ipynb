{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a7f1cb2-b466-4162-8ff9-956061753007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mysql-connector-python\n",
      "  Downloading mysql_connector_python-9.2.0-cp312-cp312-macosx_14_0_arm64.whl.metadata (6.0 kB)\n",
      "Downloading mysql_connector_python-9.2.0-cp312-cp312-macosx_14_0_arm64.whl (15.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.1/15.1 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: mysql-connector-python\n",
      "Successfully installed mysql-connector-python-9.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install mysql-connector-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f7e27ecf-fd6f-4456-8bce-27ff4d16f0a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ID                                              Title  \\\n",
      "0    1  Data Engineering with Alteryx: Helping data en...   \n",
      "1    2  Data Engineering with AWS: Acquire the skills ...   \n",
      "2    3  Data Engineering with AWS Cookbook: A recipe-b...   \n",
      "3    4  Getting Started with DuckDB: A practical guide...   \n",
      "4    5  Fundamentals of Data Engineering: Plan and Bui...   \n",
      "5    6  Designing Data-Intensive Applications: The Big...   \n",
      "6    7  Data Engineering with AWS: Acquire the skills ...   \n",
      "7    9  Financial Data Engineering: Design and Build D...   \n",
      "8   10  AI Engineering: Building Applications with Fou...   \n",
      "9   11  Data Pipelines Pocket Reference: Moving and Pr...   \n",
      "10  12  Data Engineering Best Practices: Architect rob...   \n",
      "11  14  Cracking the Data Engineering Interview: Land ...   \n",
      "12  16  Data Engineering with Databricks Cookbook: Bui...   \n",
      "13  17  Cracking the Data Engineering Interview: Land ...   \n",
      "14  18                         Snowflake Data Engineering   \n",
      "\n",
      "                                       Author   Publication Date  Rating  \\\n",
      "0                               Paul Houghton      June 30, 2022     4.3   \n",
      "1                                Gareth Eagar   October 31, 2023     4.3   \n",
      "2   Trâm Ngọc Phạm, Gonzalo Herreros González  November 29, 2024     5.0   \n",
      "3                   Simon Aubury, Ned Letcher      June 24, 2024     3.4   \n",
      "4                      Joe Reis, Matt Housley      July 26, 2022     4.7   \n",
      "5                            Martin Kleppmann      April 2, 2017     4.8   \n",
      "6                                Gareth Eagar   October 31, 2023     4.3   \n",
      "7                              Tamer Khraisha  November 12, 2024     5.0   \n",
      "8                                  Chip Huyen    January 7, 2025     4.6   \n",
      "9                              James Densmore     March 16, 2021     4.6   \n",
      "10      Richard J. Schiller, David Larochelle   October 11, 2024     5.0   \n",
      "11                             Kedeisha Bryan   November 7, 2023     4.0   \n",
      "12                              Pulkit Chadha       May 31, 2024     4.4   \n",
      "13                             Kedeisha Bryan   November 7, 2023     4.0   \n",
      "14                                 Maja Ferle   January 28, 2025     4.7   \n",
      "\n",
      "    Price  \n",
      "0    31.0  \n",
      "1    24.0  \n",
      "2    49.0  \n",
      "3    35.0  \n",
      "4    43.0  \n",
      "5    40.0  \n",
      "6    24.0  \n",
      "7    53.0  \n",
      "8    59.0  \n",
      "9    17.0  \n",
      "10   39.0  \n",
      "11   26.0  \n",
      "12   39.0  \n",
      "13   26.0  \n",
      "14   49.0  \n",
      "Data successfully stored in MySQL and sorted data saved to CSV.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import csv\n",
    "import mysql.connector\n",
    "\n",
    "# Amazon URL for Data Engineering Books\n",
    "URL = \"https://www.amazon.com/s?k=data+engineering+books\"\n",
    "\n",
    "# User-Agent list to avoid detection\n",
    "USER_AGENTS = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.0.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.83 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36\"\n",
    "]\n",
    "\n",
    "# Headers to mimic a real browser request\n",
    "HEADERS = {\n",
    "    \"User-Agent\": random.choice(USER_AGENTS),\n",
    "    \"Accept-Language\": \"en-US,en;q=0.5\"\n",
    "}\n",
    "\n",
    "def get_publication_date_from_product_page(book_url):\n",
    "    \"\"\"Fetch the publication date from a book's product page.\"\"\"\n",
    "    try:\n",
    "        product_response = requests.get(book_url, headers=HEADERS)\n",
    "        if product_response.status_code == 200:\n",
    "            product_soup = BeautifulSoup(product_response.text, \"html.parser\")\n",
    "            possible_locations = [\n",
    "                \"#detailBullets_feature_div\",\n",
    "                \"#productDetailsTable\",\n",
    "                \"#prodDetails\"\n",
    "            ]\n",
    "            for location in possible_locations:\n",
    "                details = product_soup.select_one(location)\n",
    "                if details:\n",
    "                    detail_text = details.get_text(strip=True)\n",
    "                    date_match = re.search(r\"(January|February|March|April|May|June|July|August|September|October|November|December) \\d{1,2}, \\d{4}\", detail_text)\n",
    "                    if date_match:\n",
    "                        return date_match.group()\n",
    "            \n",
    "            year_match = re.search(r\"(19|20)\\d{2}\", product_soup.text)\n",
    "            if year_match:\n",
    "                return year_match.group()\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching product page: {e}\")\n",
    "    return None\n",
    "\n",
    "# Fetch main search results page\n",
    "response = requests.get(URL, headers=HEADERS)\n",
    "\n",
    "titles, authors, pub_dates, ratings, prices = [], [], [], [], []\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    books = soup.find_all(\"div\", {\"data-component-type\": \"s-search-result\"})\n",
    "    \n",
    "    for book in books[:25]:\n",
    "        title_tag = book.find(\"h2\", class_=\"a-size-base-plus a-spacing-none a-color-base a-text-normal\")\n",
    "        title = title_tag.text.strip() if title_tag else None\n",
    "        \n",
    "        author_tag = book.find(\"div\", class_=\"a-row a-size-base a-color-secondary\")\n",
    "        if author_tag:\n",
    "            author_links = author_tag.find_all(\"a\")\n",
    "            author = \", \".join([a.text.strip() for a in author_links]) if author_links else None\n",
    "        else:\n",
    "            author = None\n",
    "\n",
    "        pub_date = None\n",
    "        date_spans = book.find_all(\"span\", class_=\"a-size-base a-color-secondary\")\n",
    "        for span in date_spans:\n",
    "            span_text = span.get_text(strip=True)\n",
    "            date_match = re.search(r\"(January|February|March|April|May|June|July|August|September|October|November|December) \\d{1,2}, \\d{4}\", span_text)\n",
    "            if date_match:\n",
    "                pub_date = date_match.group()\n",
    "                break\n",
    "        \n",
    "        if pub_date is None:\n",
    "            year_match = re.search(r\"(19|20)\\d{2}\", book.text)\n",
    "            if year_match:\n",
    "                pub_date = year_match.group()\n",
    "        \n",
    "        rating_tag = book.find(\"span\", class_=\"a-icon-alt\")\n",
    "        rating = rating_tag.text.strip().split()[0] if rating_tag else None\n",
    "\n",
    "        price_tag = book.find(\"span\", class_=\"a-price-whole\")\n",
    "        if not price_tag:\n",
    "            price_tag = book.find(\"span\", class_=\"a-offscreen\")\n",
    "        price = price_tag.text.strip() if price_tag else None\n",
    "\n",
    "        book_url_tag = book.find(\"a\", class_=\"a-link-normal s-no-outline\")\n",
    "        book_url = \"https://www.amazon.com\" + book_url_tag[\"href\"] if book_url_tag else None\n",
    "\n",
    "        if pub_date is None and book_url:\n",
    "            pub_date = get_publication_date_from_product_page(book_url)\n",
    "        \n",
    "        titles.append(title)\n",
    "        authors.append(author)\n",
    "        pub_dates.append(pub_date)\n",
    "        ratings.append(rating)\n",
    "        prices.append(price)\n",
    "        time.sleep(random.uniform(1, 3))\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"Title\": titles,\n",
    "        \"Author\": authors,\n",
    "        \"Publication Date\": pub_dates,\n",
    "        \"Rating\": ratings,\n",
    "        \"Price\": prices\n",
    "    })\n",
    "    \n",
    "    df.to_csv(\"amazon_data_engineering_books.csv\", index=False, quoting=csv.QUOTE_ALL)\n",
    "\n",
    "    # MySQL Database Connection\n",
    "    conn = mysql.connector.connect(\n",
    "        host=\"localhost\",\n",
    "        user=\"root\",\n",
    "        password=\"jasonrules12\",\n",
    "        database=\"data_engineering_books\"\n",
    "    )\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Insert Data into MySQL Table with NULL filtering\n",
    "    for index, row in df.iterrows():\n",
    "        if pd.notnull(row['Title']) and pd.notnull(row['Author']) and pd.notnull(row['Publication Date']) and pd.notnull(row['Rating']) and pd.notnull(row['Price']):\n",
    "            cursor.execute(\"\"\"\n",
    "                INSERT INTO books (title, author, publication_date, rating, price)\n",
    "                VALUES (%s, %s, %s, %s, %s)\n",
    "            \"\"\", (row['Title'], row['Author'], row['Publication Date'], row['Rating'], row['Price']))\n",
    "    conn.commit()\n",
    "\n",
    "    # Retrieve only 15 valid rows\n",
    "    query = \"\"\"\n",
    "        SELECT * FROM books \n",
    "        WHERE title IS NOT NULL \n",
    "        AND author IS NOT NULL \n",
    "        AND publication_date IS NOT NULL \n",
    "        AND rating IS NOT NULL \n",
    "        AND price IS NOT NULL\n",
    "        LIMIT 15;\n",
    "    \"\"\"\n",
    "    cursor.execute(query)\n",
    "    results = cursor.fetchall()\n",
    "    df_result = pd.DataFrame(results, columns=[\"ID\", \"Title\", \"Author\", \"Publication Date\", \"Rating\", \"Price\"])\n",
    "    print(df_result)\n",
    "    df_result.to_csv(\"sorted_books.csv\", index=False)\n",
    "\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    print(\"Data successfully stored in MySQL and sorted data saved to CSV.\")\n",
    "else:\n",
    "    print(\"Failed to retrieve webpage. Amazon may have blocked the request.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dca7b68-6846-49ba-8516-a2d80c49beee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
