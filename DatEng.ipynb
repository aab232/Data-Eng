{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a7f1cb2-b466-4162-8ff9-956061753007",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8a7f1cb2-b466-4162-8ff9-956061753007",
    "outputId": "3d6216a0-3516-4285-d509-fb857ee628d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mysql-connector-python\n",
      "  Downloading mysql_connector_python-9.2.0-cp312-cp312-win_amd64.whl.metadata (6.2 kB)\n",
      "Downloading mysql_connector_python-9.2.0-cp312-cp312-win_amd64.whl (16.1 MB)\n",
      "   ---------------------------------------- 0.0/16.1 MB ? eta -:--:--\n",
      "   -------------- ------------------------- 5.8/16.1 MB 32.0 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 12.8/16.1 MB 33.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  16.0/16.1 MB 30.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 16.1/16.1 MB 24.1 MB/s eta 0:00:00\n",
      "Installing collected packages: mysql-connector-python\n",
      "Successfully installed mysql-connector-python-9.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install mysql-connector-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7e27ecf-fd6f-4456-8bce-27ff4d16f0a6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f7e27ecf-fd6f-4456-8bce-27ff4d16f0a6",
    "outputId": "cf57ab38-66ad-4f0f-e8d2-5cafa77164fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "database error: 1045 (28000): Access denied for user 'root'@'localhost' (using password: YES)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cursor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 148\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatabase error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdb_error\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m--> 148\u001b[0m     cursor\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m    149\u001b[0m     \u001b[38;5;66;03m# closes connection to database\u001b[39;00m\n\u001b[0;32m    150\u001b[0m     conn\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cursor' is not defined"
     ]
    }
   ],
   "source": [
    "import requests  # makes http requests\n",
    "from bs4 import BeautifulSoup  # parses html content\n",
    "import pandas as pd  # data manipulation and storage\n",
    "import time  # delays between requests\n",
    "import random  # adds variability in request delays\n",
    "import re  # pattern matching in text\n",
    "import mysql.connector\n",
    "\n",
    "# amazon url for data engineering books\n",
    "URL = \"https://www.amazon.com/s?k=data+engineering+books\"\n",
    "\n",
    "# user-agent list to avoid detection by amazon's anti-scraping measures\n",
    "USER_AGENTS = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.0.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.83 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36\"\n",
    "]\n",
    "\n",
    "# headers to mimic a real browser request\n",
    "HEADERS = {\n",
    "    \"User-Agent\": random.choice(USER_AGENTS),  # randomly selects a user-agent to reduce detection risk\n",
    "    \"Accept-Language\": \"en-US,en;q=0.5\"  # specifies the preferred language for the response\n",
    "}\n",
    "\n",
    "def get_publication_date_from_product_page(book_url):\n",
    "    \"\"\"fetches the publication date from a book's product page.\"\"\"\n",
    "    try:\n",
    "        product_response = requests.get(book_url, headers=HEADERS)  # sends request to the book's product page\n",
    "        if product_response.status_code == 200:  # checks if request was successful\n",
    "            product_soup = BeautifulSoup(product_response.text, \"html.parser\")  # parses html content\n",
    "            possible_locations = [  # defines possible locations where publication date might be found\n",
    "                \"#detailBullets_feature_div\",\n",
    "                \"#productDetailsTable\",\n",
    "                \"#prodDetails\"\n",
    "            ]\n",
    "            for location in possible_locations:\n",
    "                details = product_soup.select_one(location)  # selects the first matching html element\n",
    "                if details:\n",
    "                    detail_text = details.get_text(strip=True)  # extracts text and removes extra spaces\n",
    "                    date_match = re.search(r\"(January|February|March|April|May|June|July|August|September|October|November|December) \\d{1,2}, \\d{4}\", detail_text)\n",
    "                    if date_match:\n",
    "                        return date_match.group()  # returns the matched publication date\n",
    "\n",
    "            year_match = re.search(r\"(19|20)\\d{2}\", product_soup.text)  # searches for a 4-digit year in the text\n",
    "            if year_match:\n",
    "                return year_match.group()  # returns the matched year\n",
    "    except Exception as e:\n",
    "        print(f\"error fetching product page: {e}\")  # prints error message if fetching fails\n",
    "    return None  # returns none if no publication date is found\n",
    "\n",
    "response = requests.get(URL, headers=HEADERS)  # sends a request to amazon\n",
    "\n",
    "if response.status_code == 200:  # checks if request was successful\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")  # parses html content\n",
    "    books = soup.find_all(\"div\", {\"data-component-type\": \"s-search-result\"})  # finds all book listings\n",
    "\n",
    "    # lists to store extracted book data\n",
    "    titles, authors, pub_dates, ratings, prices = [], [], [], [], []\n",
    "\n",
    "    for book in books[:50]:  # loops through the first 50 book listings\n",
    "        title_tag = book.find(\"h2\", class_=\"a-size-base-plus a-spacing-none a-color-base a-text-normal\")  # finds book title\n",
    "        title = title_tag.text.strip() if title_tag else \"unknown\"  # extracts title text or assigns \"unknown\"\n",
    "\n",
    "        author_tag = book.find(\"div\", class_=\"a-row a-size-base a-color-secondary\")  # finds author information\n",
    "        author_links = author_tag.find_all(\"a\") if author_tag else []  # gets all author links if present\n",
    "        author = \", \".join([a.text.strip() for a in author_links]) if author_links else \"unknown\"  # extracts author names\n",
    "\n",
    "        pub_date = None  # initializes publication date variable\n",
    "        date_spans = book.find_all(\"span\", class_=\"a-size-base a-color-secondary\")  # finds potential publication date elements\n",
    "        for span in date_spans:\n",
    "            span_text = span.get_text(strip=True)  # extracts text content\n",
    "            date_match = re.search(r\"(January|February|March|April|May|June|July|August|September|October|November|December) \\d{1,2}, \\d{4}\", span_text)\n",
    "            if date_match:\n",
    "                pub_date = date_match.group()  # assigns found publication date\n",
    "                break\n",
    "\n",
    "        if pub_date is None:\n",
    "            year_match = re.search(r\"(19|20)\\d{2}\", book.text)  # searches for a 4-digit year\n",
    "            if year_match:\n",
    "                pub_date = year_match.group()  # assigns found year\n",
    "\n",
    "        rating_tag = book.find(\"span\", class_=\"a-icon-alt\")  # finds book rating\n",
    "        rating = rating_tag.text.strip().split()[0] if rating_tag else \"no rating\"  # extracts rating value\n",
    "\n",
    "        price_tag = book.find(\"span\", class_=\"a-price-whole\") or book.find(\"span\", class_=\"a-offscreen\")  # finds price\n",
    "        price = price_tag.text.strip() if price_tag else \"not available\"  # extracts price or assigns \"not available\"\n",
    "\n",
    "        book_url_tag = book.find(\"a\", class_=\"a-link-normal s-no-outline\")  # finds book url\n",
    "        book_url = \"https://www.amazon.com\" + book_url_tag[\"href\"] if book_url_tag else None  # constructs full url\n",
    "\n",
    "        if pub_date is None and book_url:\n",
    "            pub_date = get_publication_date_from_product_page(book_url)  # gets publication date from product page if missing\n",
    "\n",
    "        # appends extracted data to lists\n",
    "        titles.append(title)\n",
    "        authors.append(author)\n",
    "        pub_dates.append(pub_date)\n",
    "        ratings.append(rating)\n",
    "        prices.append(price)\n",
    "        time.sleep(random.uniform(1, 3))  # adds random delay to avoid detection\n",
    "\n",
    "    # creates a pandas dataframe with collected book data\n",
    "    df = pd.DataFrame({\n",
    "        \"Title\": titles,\n",
    "        \"Author\": authors,\n",
    "        \"Publication Date\": pub_dates,\n",
    "        \"Rating\": ratings,\n",
    "        \"Price\": prices\n",
    "    })\n",
    "\n",
    "    # saves data to csv file\n",
    "    df.to_csv(\"amazon_data_engineering_books.csv\", index=False)\n",
    "    # confirmation message to user if data is successfully saved\n",
    "    print(\"Data has been saved to 'data_engineering_books.csv'\")\n",
    "\n",
    "\n",
    "    \n",
    "    try:\n",
    "        # establishes database connection\n",
    "        conn = mysql.connector.connect(\n",
    "            host=\"localhost\",\n",
    "            user=\"root\",\n",
    "            password=\"jasonrules12\",\n",
    "            database=\"data_engineering_books\"\n",
    "        )\n",
    "        # creates cursor to interact with database\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        for index, row in df.iterrows():  # iterates through dataframe rows\n",
    "            if all(pd.notnull(val) and val != \"unknown\" for val in row):  # checks if row data is valid\n",
    "                cursor.execute(\"\"\"\n",
    "                    INSERT INTO books (title, author, publication_date, rating, price)\n",
    "                    VALUES (%s, %s, %s, %s, %s)\n",
    "                \"\"\", (row['Title'], row['Author'], row['Publication Date'], row['Rating'], row['Price']))  # inserts data\n",
    "        conn.commit()  # commits transaction\n",
    "\n",
    "        # query that extract only 3 columns from the table and sorts table based on rating column\n",
    "        query = \"\"\"\n",
    "            SELECT title, author, price\n",
    "            FROM books\n",
    "            ORDER BY rating DESC;\n",
    "        \"\"\"\n",
    "        cursor.execute(query) # executes query\n",
    "        results = cursor.fetchall() # fetches all results\n",
    "        df_result = pd.DataFrame(results, columns=[\"Title\", \"Author\", \"Price\", \"Rating\"]) # converts results into a dataframe\n",
    "        print(df_result) # prints results\n",
    "\n",
    "    # handles database errors\n",
    "    except mysql.connector.Error as db_error:\n",
    "        print(f\"database error: {db_error}\")\n",
    "\n",
    "    finally:\n",
    "        cursor.close()\n",
    "        # closes connection to database\n",
    "        conn.close()\n",
    "        print(\"\\nThese are the top-rated data engineering books our shoppers love ♡. \\nHappy reads!\")\n",
    "\n",
    "else:\n",
    "    # error handling message for user if parsing fails\n",
    "    print(\"ⓘ Failed to retrieve webpage. Amazon may have blocked the request. :(\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79970839-27fe-4167-9384-2be0f4387fcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
